"""
EMA (Engineering Mathematics Assistant) ë©”ì¸ ì‹¤í–‰ íŒŒì¼
"""
import os
import uuid
import warnings
import re, json
from fastapi import FastAPI, HTTPException, status
from pydantic import BaseModel, Field
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig
from workflow import graph
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings


warnings.filterwarnings("ignore", message="Convert_system_message_to_human will be deprecated!")
warnings.filterwarnings("ignore", message=".*get_relevant_documents.*")
# ì„¤ì •
config = RunnableConfig(recursion_limit=10, configurable={"thread_id": str(uuid.uuid4())})

def process_query(query: str) -> str:
    """
    ì‚¬ìš©ì ì§ˆì˜ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        query (str): ì‚¬ìš©ìì˜ ì§ˆì˜

    Returns:
        str: ì‹œìŠ¤í…œì˜ ì‘ë‹µ
    """
    print(f"ì‚¬ìš©ì ì§ˆì˜ ì²˜ë¦¬ ì‹œì‘: {query}")

    state = {"messages": [HumanMessage(content=query, name="User")]}

    try:
        for step in graph.stream(state, config=config):
            if step:
                node_name = list(step.keys())[0]
                print(f"ğŸ”„ ì‹¤í–‰ ì¤‘: {node_name}")

        final_state = graph.get_state(config=config)
        messages = final_state.values['messages']

        ai_messages = [msg for msg in messages if isinstance(msg, AIMessage)]
        if ai_messages:
            final_message = ai_messages[-1].content
        else:
            final_message = messages[-1].content if messages else "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        return final_message

    except Exception as e:
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
llm = ChatGoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=GOOGLE_API_KEY,
    convert_system_message_to_human=True,
    temperature=0.2
)
app = FastAPI(
    title="Calc-Question Generator API",
    description="ë¯¸ì ë¶„ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ëŠ” REST API",
    version="1.0.0",
)

# ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡
# ë¬¸ì œìƒì„± api
# ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡
class QuestionRequest(BaseModel):
    topics: str           = Field(..., example="í•¨ìˆ˜ì˜ ê·¹í•œ í™•ì¸ë¬¸ì œ")
    range_: str           = Field(..., alias="range", example="2.2 The Limit of Functions")
    summarized: str       = Field(..., example="ê·¹í•œì˜ ì •ì˜, í•œìª½Â·ë¬´í•œ ê·¹í•œ, ìˆ˜ì§ ì ê·¼ì„ ")
    difficulty: str       = Field(..., example="í’€ì´ 5ì¤„ ì´ë‚´")
    quiz_examples: str    = Field(..., example="(ì˜ˆì‹œ ë¬¸ì œ)")

class QuestionResponse(BaseModel):
    question: str
    choice1: str
    choice2: str
    choice3: str
    choice4: str
    choice5: str
    answer: int
    solution: str

@app.post(
    "/questions",
    response_model=QuestionResponse,
    summary="ê°ê´€ì‹ ë¬¸ì œ ìƒì„±",
    status_code=status.HTTP_201_CREATED,
)
async def create_question(payload: QuestionRequest):
    """
    LangChain + OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•´ ê°ê´€ì‹ ë¬¸ì œë¥¼ ìƒì„±í•˜ì—¬ JSON ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        # 1) payload â†’ JSON string
        payload_str = payload.model_dump_json(by_alias=True)
        print("***** payload_str = ", payload_str)
        # 2) 1ì°¨ LLM ìš”ì²­: â€œë¬¸ì œ ìƒì„±â€
        query = (
            "ë‹¤ìŒ JSONì„ ê¸°ë°˜ìœ¼ë¡œ, ê°ê´€ì‹ ë¬¸ì œë¥¼ â€˜ì‚¬ëŒì´ ì½ê¸° í¸í•œâ€™ í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì¤˜.\n"
            f"{payload_str}"
        )
        print("***** query = ", query)
        raw_result: str = process_query(query)
        print("***** raw_result = ", raw_result)
        # 3) 2ì°¨ LLM ìš”ì²­: â€œJSONìœ¼ë¡œ ë³€í™˜â€
        json_prompt = (
            "ìœ„ì—ì„œ ìƒì„±ëœ ë¬¸ì œë¥¼, ì•„ë˜ ìŠ¤í‚¤ë§ˆì— ë§ì¶° **ìˆœìˆ˜ JSON**ìœ¼ë¡œë§Œ ë³€í™˜í•´ì¤˜.\n"
            "í‚¤: question, choice1~choice5, answer(ì •ë‹µ ë²ˆí˜¸, 1~5), solution\n\n"
            f"{raw_result}"
        )
        response = await llm.agenerate([[HumanMessage(content=json_prompt)]])
        print("***** response = ", response)
        json_str = response.generations[0][0].text.strip()
        print("***** json_str = ", json_str)
        # 3-4) JSON â†’ Pydantic ëª¨ë¸
        match = re.search(r"```(?:json)?\s*(\{.*\})\s*```", json_str, re.S)
        clean_json = match.group(1) if match else json_str
        data = json.loads(clean_json)
        return QuestionResponse(**data)

    except Exception as e:
        # í•„ìš” ì‹œ ì„¸ë¶„í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )

# ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡
# ì§ˆì˜ì‘ë‹µ api
# ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡ã…¡
class QARequest(BaseModel):
    query: str = Field(..., example="í•¨ìˆ˜ì˜ ê·¹í•œì´ë€ ë¬´ì—‡ì¸ê°€ìš”?")

class QAResponse(BaseModel):
    answer: str

@app.post(
    "/qna",
    response_model=QAResponse,
    summary="ì‚¬ìš©ì ì§ˆì˜ì‘ë‹µ",
    status_code=status.HTTP_200_OK,
)
async def answer_query(payload: QARequest):
    """
    ì‚¬ìš©ìë¡œë¶€í„° ë°›ì€ ì§ˆì˜ë¥¼ AI ê·¸ë˜í”„ì— ì „ë‹¬í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    try:
        result = process_query(payload.query)
        return QAResponse(answer=result)
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e),
        )



if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True, log_level="debug")
